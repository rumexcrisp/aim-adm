{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgaben Linear Networks\n",
    "========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the common imports:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you should create a linear regression\n",
    "model from scratch and test it on some synthetically created data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  \n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "n_samples=100\n",
    "X, y = synthetic_data(true_w, true_b, n_samples)\n",
    "K = 2\n",
    "\n",
    "print(\"Training data \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to fit a simple regression model with Batch Gradient Descent.\n",
    "We start with randomly chosen values for the weights and zero bias.\n",
    "First, implement the function below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "w = torch.normal(0, 0.01, size=(K, 1))\n",
    "b = torch.zeros(1)\n",
    "\n",
    "def linreg(X, w, b):\n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the loss functions to be used:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    \"\"\" Compute the sum of the quadratic errors \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement the training loop for Gradient Descent.\n",
    "You should not use `autograd` for computing the gradient, instead\n",
    "build on the closed formula presented in the lecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = np.zeros(n_epoch) # to record current loss\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    pass\n",
    "    # 1.  Compute the prediction y_hat\n",
    "\n",
    "    # remember the loss for plotting it later\n",
    "    # loss_arr[i] = squared_loss(y_hat, y)\n",
    "\n",
    "    # 2. Use y_hat and y to compute the gradients\n",
    "\n",
    "\n",
    "    # 3. Update the parameters\n",
    "\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss and linear regression model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "def squared_loss(y_hat, y):\n",
    "    assert y_hat.shape == y.shape\n",
    "    return 0.5 * ((y_hat -y)**2).sum()\n",
    "\n",
    "def linreg(X, w, b):\n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "# Note that w has K rows and 1 column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the lecture, the gradient of the $i$-th  example is given by:\n",
    "$$\\nabla L^{(i)}(\\mathbf \\theta) = \\left(\\hat y^{(i)} -  y^{(i)}\\right) \\cdot \\begin{pmatrix} 1 \\\\ x^{(i)}_1 \\\\ ... \\\\ x^{(i)}_d \\end{pmatrix}$$\n",
    "Remember that here $\\mathbf \\theta = (\\theta_0, \\theta_1, ..., \\theta_d)$. \n",
    "\n",
    "In our example we have $d=K=2$ and we compute the gradient of $\\theta_0$ (here $b$) seperately. \n",
    "Also we want to compute the average gradient over the whole batch.\n",
    "\n",
    "For a batch we have\n",
    "$$\n",
    "\\hat {\\textbf y} - \\textbf y  = \\begin{pmatrix} \\hat y^{(1)} - y^{(1)} \\\\ \\vdots \\\\ \\hat y^{(N)} - y^{(N)} \\end{pmatrix} \\quad\\text{and}\\quad\n",
    "\\textbf X = \\begin{pmatrix} x_1^{(1)} &  x_2^{(1)} \\\\ \\vdots \\\\  x_1^{(N)} & x_2^{(N)}\\end{pmatrix} \n",
    "$$\n",
    "Hence, we have to multiply them point wise and average the results in vertical direction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation for the weights (th_1 and th_2)\n",
    "def gradient_W(y_hat, y):\n",
    "    return torch.mul((y_hat -y), X).mean(axis=0).reshape(2,1)\n",
    "    # reshape to match w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = torch.normal(0, 0.01, size=(K, 1))\n",
    "b = torch.zeros(1)\n",
    "\n",
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = torch.zeros(n_epoch)\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    y_hat = linreg(X, w, b)\n",
    "\n",
    "    loss_arr[i] = squared_loss(y_hat, y)\n",
    "    \n",
    "    grad_w = torch.mul((y_hat -y ),X).mean(axis=0).reshape(K, 1)\n",
    "    grad_b = (y_hat - y).mean() \n",
    "    \n",
    "    w -= step*grad_w\n",
    "    b -= step*grad_b\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss');\n",
    "plt.yscale('log')\n",
    "plt.plot(loss_arr);\n",
    "print(\"W=\", w)\n",
    "print(\"b=\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear networks with autograd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal now is to use `autograd` the compute the gradient.\n",
    "You can use the same skeleton as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "w = torch.normal(0, 0.01, size=(K, 1))\n",
    "b = torch.zeros(1)\n",
    "\n",
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = np.zeros(n_epoch) # to record current loss\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    pass\n",
    "    # 1.  Compute the prediction y_hat\n",
    "\n",
    "    # loss = squared_loss(y_hat, y)\n",
    "\n",
    "    # remember the loss for plotting it later\n",
    "    # loss_arr[i] = loss.detach().numpy()\n",
    "\n",
    "    # 2. Use the computed loss to compute the gradients\n",
    "    # 3. Update the parameters, remember to zero the gradients\n",
    "\n",
    "plt.plot(loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = torch.zeros(n_epoch) # to record current loss\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    y_hat = linreg(X, w, b)\n",
    "    loss = squared_loss(y_hat, y)\n",
    "    \n",
    "    # remember the loss for plotting it later\n",
    "    loss_arr[i] = loss.detach()\n",
    "\n",
    "    # 2. Use the computed loss to compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 3. Update the parameters\n",
    "    with torch.no_grad():\n",
    "        w -= step*w.grad/n_samples\n",
    "        b -= step*b.grad/n_samples\n",
    "        \n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')\n",
    "plt.plot(loss_arr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  want to implement a linear network for classification.\n",
    "We use the famous IRIS data set as an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the network is implemented as a class,\n",
    "the only thing missing is the implementation of the softmax function,\n",
    "for example\n",
    "$$\n",
    "softmax(\\textbf y)_1 = \\frac{e^{y_1}}{ \\sum_{i=1}^3  e^{y_i} }.\n",
    "$$\n",
    "You have to implement it below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    pass\n",
    "\n",
    "\n",
    "class SoftmaxNetwork:\n",
    "\n",
    "    def __init__(self, num_input, num_output, dtype=torch.float64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_input: dimension of input space\n",
    "            num_output: number if output classes\n",
    "        \"\"\"\n",
    "        self.w = torch.randn((num_input, num_output),\n",
    "                             dtype=dtype).requires_grad_(True)\n",
    "        \n",
    "        self.b = torch.randn(num_output, dtype=dtype).requires_grad_(True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: tensor of shape (n, d)\n",
    "        \"\"\"\n",
    "        y = (X @ self.w + self.b)\n",
    "        return softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to implement the cross entropy loss, it is already finished:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return (-torch.log(y_hat[range(len(y_hat)), y])).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this implementation does not require a one-hot-encoding for $y$\n",
    "(but there is one side effect: $y$ has to be of type `torch.int64`!).\n",
    "\n",
    "The final step is to implement a function that runs the training for us:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(net, X, y, f_loss, n_epochs, lr=0.1):\n",
    "    \"\"\" Run the training.\n",
    "    Args:\n",
    "       net: an instance of SoftmaxNetwork\n",
    "       X, y: training data\n",
    "       f_loss: the loss function\n",
    "       n_epochs: number of epochs\n",
    "       lr: the learning rate\n",
    "\n",
    "    Returns:\n",
    "      training loss: np.array (loss per epoch)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model, don't forget to cast X and y to Pytorch tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cast X and y\n",
    "\n",
    "net = SoftmaxNetwork(4,3)\n",
    "train_loss = run_training(net, X, y,  cross_entropy, n_epochs=100, lr=0.2)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Learning curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Run the training several times, and observe the different learning curves.\n",
    "2.  Try the same with a lower learning rate, say $lr=0.05$. Do you see any differences?\n",
    "\n",
    "Finally check the accuracy of the model, that is the fraction of correctly predicted examples.\n",
    "Of course this is on training only. If you like you can try to split the\n",
    "data into train and test and evaluate your network on the test data set.\n",
    "A useful function for this is `train_test_split` found in `sklearn.model_selection`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and implement the network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "n_epochs = 30\n",
    "\n",
    "def softmax(y):\n",
    "    y_exp = torch.exp(y)\n",
    "    partition = y_exp.sum(axis=1, keepdims=True)\n",
    "    return y_exp / partition\n",
    "\n",
    "\n",
    "class SoftmaxNetwork:\n",
    "\n",
    "    def __init__(self, num_input, num_output, dtype=torch.float64):\n",
    "        self.w = torch.randn((num_input, num_output),\n",
    "                             dtype=dtype).requires_grad_(True)\n",
    "        self.b = torch.randn(num_output, dtype=dtype).requires_grad_(True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: tensor of shape (n, d)\n",
    "        \"\"\"\n",
    "        y = (X @ self.w + self.b)\n",
    "        return softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training loop and the loss function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(net, X, y, f_loss, n_epochs, lr=0.1):\n",
    "    train_loss = np.zeros(n_epochs)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        yhat = net.forward(X)\n",
    "        l = f_loss(yhat, y)\n",
    "        l.backward()\n",
    "        with torch.no_grad():\n",
    "            net.w -= lr*net.w.grad\n",
    "            net.b -= lr*net.b.grad\n",
    "            net.w.grad.zero_()\n",
    "            net.b.grad.zero_()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            yhat = net.forward(X)\n",
    "            l = f_loss(yhat, y)\n",
    "            \n",
    "        train_loss[epoch] = l\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    return (-torch.log(y_hat[range(len(y_hat)), y])).mean()\n",
    "\n",
    "net = SoftmaxNetwork(4,3, dtype=torch.float32)\n",
    "\n",
    "train_loss = run_training(net, X, y,  cross_entropy, 100, lr=0.2)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.yscale('log')\n",
    "plt.title('Learning curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We observe an oscillating behaviour,\n",
    "we run it next with a smaller learning rate:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SoftmaxNetwork(4,3, dtype=torch.float32)\n",
    "train_loss = run_training(net, X, y,  cross_entropy, 100, lr=0.05)\n",
    "\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.yscale('log')\n",
    "plt.title('Learning curve');\n",
    "plt.plot(train_loss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
