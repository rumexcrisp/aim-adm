{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgaben Linear Networks\n",
    "========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the common imports:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you should create a linear regression\n",
    "model from scratch and test it on some synthetically created data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "\n",
    "n_samples=100\n",
    "X, y = synthetic_data(true_w, true_b, n_samples)\n",
    "K = 2\n",
    "\n",
    "print(f\"Training data {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to fit a simple regression model with Batch Gradient Descent.\n",
    "We start with randomly chosen values for the weights and zero bias.\n",
    "First, implement the function below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "w = torch.normal(0, 0.01, size=(K, 1))\n",
    "b = torch.zeros(1)\n",
    "\n",
    "def linreg(X, w, b):\n",
    "    \"\"\"The linear regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Input features with shape (m, n), where m is the number of samples and n is the number of features.\n",
    "    w (numpy.ndarray): Weight vector with shape (n, 1).\n",
    "    b (float): Bias term.\n",
    "    \n",
    "    Returns:\n",
    "    predictions (numpy.ndarray): Predicted values for each input sample, shape (m, 1).\n",
    "    \"\"\"\n",
    "    return torch.matmul(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the loss functions to be used:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    \"\"\"\n",
    "    Calculate the squared loss (mean squared error) between predicted values and actual values.\n",
    "\n",
    "    Parameters:\n",
    "    y_hat (numpy.ndarray): Predicted values with shape (m, 1), where m is the number of samples.\n",
    "    y (numpy.ndarray): Actual target values with shape (m, 1), matching the shape of y_hat.\n",
    "\n",
    "    Returns:\n",
    "    loss (float): Mean squared error loss.\n",
    "    \"\"\"\n",
    "    assert y_hat.shape == y.shape\n",
    "    return 0.5 * ((y_hat - y)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement the training loop for Gradient Descent.\n",
    "You should not use `autograd` for computing the gradient, instead\n",
    "build on the closed formula presented in the lecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation for the weights (th_1 and th_2)\n",
    "def gradient_W(y_hat, y):\n",
    "    return torch.mul((y_hat - y), X).mean(axis=0).reshape(2, 1) # reshape to match w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = np.zeros(n_epoch) # to record current loss\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    # 1.  Compute the prediction y_hat\n",
    "    y_hat = linreg(X, w, b)\n",
    "\n",
    "    # remember the loss for plotting it later\n",
    "    # loss_arr[i] = squared_loss(y_hat, y)\n",
    "    loss = squared_loss(y_hat, y)\n",
    "    loss_arr[i] = loss\n",
    "\n",
    "    # 2. Use y_hat and y to compute the gradients\n",
    "    grad_w = torch.mul((y_hat - y), X).mean(axis=0).reshape(K, 1)\n",
    "    grad_b = (y_hat - y).mean()\n",
    "\n",
    "    # 3. Update the parameters\n",
    "    w -= step * grad_w\n",
    "    b -= step * grad_b\n",
    "\n",
    "plt.plot(loss_arr)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "print(f\"W={w}\")\n",
    "print(f\"b={b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear networks with autograd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal now is to use `autograd` the compute the gradient.\n",
    "You can use the same skeleton as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "w = torch.normal(0, 0.01, size=(K, 1), requires_grad=True)\n",
    "b = torch.zeros(1)\n",
    "\n",
    "step = 0.1\n",
    "n_epoch = 100\n",
    "\n",
    "loss_arr = np.zeros(n_epoch) # to record current loss\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    pass\n",
    "    # 1.  Compute the prediction y_hat\n",
    "    y_hat = linreg(X, w, b)\n",
    "    loss = squared_loss(y_hat, y)\n",
    "\n",
    "    # remember the loss for plotting it later\n",
    "    loss_arr[i] = loss.detach()\n",
    "\n",
    "    # 2. Use the computed loss to compute the gradients\n",
    "    # 3. Update the parameters, remember to zero the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= step * w.grad/n_samples\n",
    "        b -= step * b.grad/n_samples\n",
    "\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "plt.plot(loss_arr)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "print(f\"W={w}\")\n",
    "print(f\"b={b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  want to implement a linear network for classification.\n",
    "We use the famous IRIS data set as an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the network is implemented as a class,\n",
    "the only thing missing is the implementation of the softmax function,\n",
    "for example\n",
    "$$\n",
    "softmax(y)_1 = \\frac{e^{y_1}}{ \\sum_{i=1}^3  e^{y_i} }.\n",
    "$$\n",
    "You have to implement it below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    y_exp = torch.exp(y)\n",
    "    partition = y_exp.sum(axis=1, keepdims=True)\n",
    "    return y_exp / partition\n",
    "\n",
    "\n",
    "class SoftmaxNetwork:\n",
    "\n",
    "    def __init__(self, num_input, num_output, dtype=torch.float64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_input: dimension of input space\n",
    "            num_output: number if output classes\n",
    "        \"\"\"\n",
    "        self.w = torch.randn((num_input,num_output),\n",
    "                             dtype=dtype).requires_grad_(True)\n",
    "        self.b = torch.randn(num_output, dtype=dtype).requires_grad_(True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: tensor of shape (n, d)\n",
    "        \"\"\"\n",
    "        y = (X @ self.w + self.b)\n",
    "        return softmax(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to implement the cross entropy loss, it is already finished:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return (-torch.log(y_hat[range(len(y_hat)), y])).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this implementation does not require a one-hot-encoding for $y$\n",
    "(but there is one side effect: $y$ has to be of type `torch.int64`!).\n",
    "\n",
    "The final step is to implement a function that runs the training for us:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(net, X, y, f_loss, n_epochs, lr=0.1):\n",
    "    \"\"\" Run the training.\n",
    "    Args:\n",
    "       net: an instance of SoftmaxNetwork\n",
    "       X, y: training data\n",
    "       f_loss: the loss function\n",
    "       n_epochs: number of epochs\n",
    "       lr: the learning rate\n",
    "\n",
    "    Returns:\n",
    "      training loss: np.array (loss per epoch)\n",
    "    \"\"\"\n",
    "    train_loss = np.zeros(n_epochs)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        y_hat = net.forward(X)\n",
    "        l = f_loss(y_hat, y)\n",
    "        l.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.w -= lr*net.w.grad\n",
    "            net.b -= lr*net.b.grad\n",
    "            net.w.grad.zero_()\n",
    "            net.b.grad.zero_()\n",
    "\n",
    "            y_hat = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model, don't forget to cast X and y to Pytorch tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cast X and y\n",
    "\n",
    "net = SoftmaxNetwork(4,3)\n",
    "train_loss = run_training(net, X, y,  cross_entropy, n_epochs=100, lr=0.2)\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Learning curve');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Run the training several times, and observe the different learning curves.\n",
    "2.  Try the same with a lower learning rate, say $lr=0.05$. Do you see any differences?\n",
    "\n",
    "Finally check the accuracy of the model, that is the fraction of correctly predicted examples.\n",
    "Of course this is on training only. If you like you can try to split the\n",
    "data into train and test and evaluate your network on the test data set.\n",
    "A useful function for this is `train_test_split` found in `sklearn.model_selection`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "org": null,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
