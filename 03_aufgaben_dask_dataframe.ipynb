{"cells":[{"cell_type":"markdown","metadata":{},"source":["\n","\n","\n","**Author:** Steffen Schober\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Acknowlegment\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["This notebook is based on the DASK tutorial.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Data\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Prepare the data, make sure that `prep.py` is the same directory than this notebook.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%run 03_prep.py -d flights"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%run 03_prep.py -d accounts"]},{"cell_type":"markdown","metadata":{},"source":["## Setup\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from dask.distributed import Client\n","\n","client = Client(n_workers=4)\n","client"]},{"cell_type":"markdown","metadata":{},"source":["You can access the dashboard using your web browser, the linke is also found here:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(client.cluster.dashboard_link)"]},{"cell_type":"markdown","metadata":{},"source":["Explore the dashboard, you can find a lot of information there.\n","Note that under `Info` you find information about\n","the TCP endpoint of the scheduler (you can use this to connect to the cluster via the `Client`.).\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## First Example - Loading CSV file\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import dask\n","filename = os.path.join('data', 'accounts.*.csv')\n","filename"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import dask.dataframe as dd\n","df = dd.read_csv(filename)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load and count number of rows\n","len(df)"]},{"cell_type":"markdown","metadata":{},"source":["## Flights Data Set\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load and count number of rows\n","df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n","  \t       parse_dates={'Date': [0, 1, 2]},\n","  \t       dtype={'TailNum': str,\n","  \t\t      'CRSElapsedTime': float,\n","  \t\t      'Cancelled': bool}\n",")\n","df"]},{"cell_type":"markdown","metadata":{},"source":["Notice that the representation of the dataframe object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes.\n","We enforce the dtype for three columns, because those do not contain data in the first rows, hence,\n","type inference will fail&#x2026; (you can check this by omitting the `dtype` in `read_csv()`).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Unlike `pandas.read_csv` which reads in the entire file before inferring datatypes,\n","`dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file if using a glob).\n","These inferred datatypes are then enforced when reading all partitions.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Some Analysis\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["We compute the maximum of the `DepDelay` column. With just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums\n","\n","    maxes = []\n","    for fn in filenames:\n","        df = pd.read_csv(fn)\n","        maxes.append(df.DepDelay.max())\n","    \n","    final_max = max(maxes)\n","\n","We could wrap that `pd.read_csv` with `dask.delayed` so that it runs in parallel.\n","Regardless, we’re still having to think about loops, intermediate results (one per file) and the final reduction (max of the intermediate maxes).\n","\n","    df = pd.read_csv(filename, dtype=dtype)\n","    df.DepDelay.max()\n","\n","`dask.dataframe` lets us write pandas-like code, that operates on larger than memory datasets in parallel.\n","Here we compute the max of `DepDelay`:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%time df.DepDelay.max().compute()"]},{"cell_type":"markdown","metadata":{},"source":["Let's visualize the graph:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# notice the parallelism\n","df.DepDelay.max().visualize()"]},{"cell_type":"markdown","metadata":{},"source":["## Exercises\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Try to answer the following questions:\n","\n","1.  How many rows are in our dataset?\n","2.  In total, how many non-canceled flights were taken?\n","3.  In total, how many non-cancelled flights were taken from each airport?\n","4.  What day of the week has the worst average departure delay?\n","\n","Hint for the third question:\n","use `groupby` with the aggregate function `count`.\n","See [https://pandas.pydata.org/pandas-docs/stable/groupby.html](https://pandas.pydata.org/pandas-docs/stable/groupby.html).\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Sharing Intermediate Results\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["When computing all of the above, we sometimes did the same operation more than once.\n","For most operations, `dask.dataframe` hashes the arguments, allowing duplicate computations to be shared, and only computed once.\n","\n","For example, lets compute the mean and standard deviation for departure delay of all non-canceled flights.\n","Since dask operations are lazy, those values aren’t the final results yet. They’re just the recipe required to get the result.\n","\n","If we compute them with two calls to compute, there is no sharing of intermediate computations.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["non_cancelled = df[~df.Cancelled]\n","mean_delay = non_cancelled.DepDelay.mean()\n","std_delay = non_cancelled.DepDelay.std()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","\n","mean_delay_res = mean_delay.compute()\n","std_delay_res = std_delay.compute()"]},{"cell_type":"markdown","metadata":{},"source":["But let’s try by passing both to a single compute call.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"]},{"cell_type":"markdown","metadata":{},"source":["The task graphs for both results are merged when calling `dask.compute`, allowing shared operations to only be done once instead of twice.\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"org":null},"nbformat":4,"nbformat_minor":0}
