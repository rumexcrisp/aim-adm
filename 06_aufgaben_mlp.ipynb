{"cells":[{"cell_type":"markdown","metadata":{},"source":"Networks with Multiple Layers\n=============================\n\n"},{"cell_type":"markdown","metadata":{},"source":["## Overview\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Here we will use  the **MNIST fashion** data set for classification, available here\n[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)\nor using `torchvision.datasets.FashionMNIST`.\n\nWe will also monitor our model with the help of Tensorboard.\n\nWe will start from a Softmax Regression which we construct from scratch.\nThis is them refactored to use the standard modules from `pytorch`, e.g.,\n\n-   `torch.nn.Module`, `torch.nn.Linear`, &#x2026;\n-   `torch.optim` for optimization\n-   `DataLoader`\n\nFinally you have to construct and train a MLP.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Imports\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The *standard* imports:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport torch\nimport torchvision"]},{"cell_type":"markdown","metadata":{},"source":["## Tensorboard\n\n"]},{"cell_type":"markdown","metadata":{},"source":["For the following, you have to install **Tensorboard**. First you need to install it with\n\n    pip install tensorboard\n\nYou have to run it from the command line with\n\n    tensorboard --logdir <DIR>\n\nLater your models will write logging information to the directory `DIR`.\nFor now it is assumed that\n\n    DIR=$HOME/tmp/tensorboard\n\nTensorboard can be accessed under [http://localhost:6006](http://localhost:6006).\n\nYou will find more information here:\n[https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### FashionMNIST\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Some info from [https://pytorch.org/docs/stable/torchvision/datasets.html](https://pytorch.org/docs/stable/torchvision/datasets.html)\n> TORCHVISION.DATASETS\n> All datasets are subclasses of `torch.utils.data.Dataset` i.e, they have `__getitem__` and `__len__` methods implemented.\n> Hence, they can all be passed to a `torch.utils.data.DataLoader` which can load multiple samples\n> parallelly using `torch.multiprocessing` workers.Here is the data set we want to use:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from torchvision.datasets import FashionMNIST\n# if you want, read the doc string:\n# ?FashionMNIST"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from torchvision import transforms\ntransform = transforms.Compose([transforms.ToTensor()])\n\nfashion_train = FashionMNIST(\"~/no_backup/data_fashion/\",\n               train=True, # the default\n               download=True,\n               transform=transform\n)\n\n# this will will not be useed in to following\nfashion_test = FashionMNIST(\"~/no_backup/data_fashion/\",\n               train=False,\n               download=True,\n               transform=transform # converts to [0,1]\n)\n# if you like you can later evaluate your final model with it"]},{"cell_type":"markdown","metadata":{},"source":["Let us look at the data. Here are the classes:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fashion_train.classes"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["print (\"Type\", type(fashion_train.data))\nprint (\"dtype\", fashion_train.data.dtype)\nprint (\"Shape\", fashion_train.data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["So a big tensor, each picture is a 28x28 pixel picture.\nBut also note the `dtype` is a `torch.uint`.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fashion_train.data.float().dtype"]},{"cell_type":"markdown","metadata":{},"source":["Lets us show some examples, together with their labels:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(3,3, figsize=(12,12))\naxs = axs.flatten()\nfor i, ax in zip(range(len(axs)), axs):\n    ax.imshow(fashion_train.data[i], cmap='gray')\n    ax.set_title(fashion_train.classes[fashion_train.targets[i]])"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loaders\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The data can be passed in a model with a `DataLoader`, like shown below.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\ntrain_dl = DataLoader(fashion_train, batch_size=10, shuffle=True)\n\nprint('Number of data points', len(fashion_train))\nprint ('Number of batches', len(train_dl))\n\nfor X, y in train_dl:\n    print (X.shape)\n    print(y.shape)\n    break"]},{"cell_type":"markdown","metadata":{},"source":["## Start from Softmax Regression\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Introduction\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We will understand this as a classification problem with $28*28$ features and first apply Softmax regresssion.\nInternally we need to  reshape the data. At the end we expect a $N\\times 784$ tensor,\nwhere $N$ is the number of examples in the batch.\nWe see that `X` is of shape $N \\times d \\times d$, with $d=28$.\nTo flatten it (using the batch from the previous section) use for example\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["X.flatten(start_dim=1).shape"]},{"cell_type":"markdown","metadata":{},"source":["We start from  our solution from last time:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def softmax_old(X):\n    X_exp = torch.exp(X)\n    partition = X_exp.sum(axis=1, keepdims=True)\n    return X_exp / partition\n\ndef cross_entropy_old(y_hat, y):\n    return (-torch.log(y_hat[range(len(y_hat)), y])).mean()\n\nclass SoftmaxNetwork_old:\n\n    def __init__(self, num_input, num_output, dtype=torch.float64):\n        \"\"\"\n        Args:\n            num_input: dimension of input space\n            num_output: number if output classes\n        \"\"\"\n        self.w = torch.randn((num_input,num_output),\n                             dtype=dtype).requires_grad_(True)\n        self.b = torch.randn(num_output, dtype=dtype).requires_grad_(True)\n\n    def forward(self, X):\n        \"\"\"\n        Args:\n            X: tensor of shape (n, d)\n        \"\"\"\n        y = (X @ self.w + self.b)\n        return softmax_old(y)"]},{"cell_type":"markdown","metadata":{},"source":["### Refactor: Softmax and Cross-Entropy\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Basically an equivalent implementation of `Softmax` using PyTorch  functions is shown here:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import  torch.nn as nn\n\nsoftmax = nn.Softmax(dim=1)\n\ndef test_softmax():\n    X = torch.normal(0,1, size=(4,2))\n    # would throw an exception if assertion is wrong\n    np.testing.assert_almost_equal(\n       softmax(X).numpy(), softmax_old(X) )\n\ntest_softmax()"]},{"cell_type":"markdown","metadata":{},"source":["But due to numerical reasons it is recommended to use `LogSoftmax`:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["log_softmax = nn.LogSoftmax(dim=1)\n\ndef test_softmax2():\n    X = torch.normal(0,1, size=(4,2))\n    # would throw an exception if assertion is wrong\n    np.testing.assert_almost_equal(\n       softmax(X).numpy(), np.exp(log_softmax(X)))\n\ntest_softmax2()"]},{"cell_type":"markdown","metadata":{},"source":["Of course, now we have to change our loss function, which now works on the logarithm of the softmax:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# test it\ndef test_cross_entropy():\n    loss_func_old =cross_entropy_old\n\n    loss_func = nn.NLLLoss()\n\n    X = torch.normal(0,1, size=(4,2))\n    y = torch.tensor([1,0,1,1])\n\n    np.testing.assert_almost_equal(\n        loss_func(log_softmax(X), y).numpy(),\n        loss_func_old(softmax_old(X), y).numpy()\n    )\n\n\ntest_cross_entropy()"]},{"cell_type":"markdown","metadata":{},"source":["A second (equivalent method) is to use the `CrossEntropyLoss`:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def test_cross_entropy2():\n    loss_func = nn.NLLLoss()\n    loss_func_new = nn.CrossEntropyLoss()\n\n    # test it\n    X = torch.normal(0,1, size=(4,2))\n    y = torch.tensor([1,0,1,1])\n\n    # throws an exception if the assertion fails\n    np.testing.assert_almost_equal(\n        loss_func_new(X, y).numpy(),\n        loss_func(log_softmax(X), y).numpy())\n\ntest_cross_entropy2()"]},{"cell_type":"markdown","metadata":{},"source":["In this case the network has to return the result of the  output\nlayer without the softmax!\n\nHere is an example if a  network to be used with `NLLLoss`:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["class SoftmaxNetwork:\n    def __init__(self):\n        num_input=28*28\n        num_output=10\n        self.w = torch.randn((num_input,num_output),\n                             dtype=torch.float32).requires_grad_(True)\n        self.b = torch.randn(num_output, dtype=torch.float32).requires_grad_(True)\n\n    # New method: call operator\n    def __call__(self, X):\n        return self.forward(X)\n\n    def forward(self, X):\n        \"\"\"\n        Args:\n            X: tensor of shape (n, d, d)\n        \"\"\"\n        # flatten X!\n        y = (X.flatten(start_dim=1) @ self.w + self.b)\n        return log_softmax(y)"]},{"cell_type":"markdown","metadata":{},"source":["In the following section we will switch to the second possibility, therefore use the `CrossEntropyLoss`.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Refactor: Inherit from nn.Module\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Usually, PyTorch models inherit from `torch.nn.module`.\n`torch.nn` contains many useful modules and layers.\nFor  example a simple **linear layer** (a **fully connected layer** where the activation\nis the identity function), is provided by `nn.Linear`.\nAlso flattening can be done with `nn.Flatten`,\nwhich can be viewed as a special layer.\n\nUsing these new modules, we obtain the following class\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["class LinearNetwork(nn.Module):\n    def __init__(self, num_input=28*28, num_output=10):\n        # call constructor of super class\n        super().__init__()\n\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(num_input, num_output)\n\n    def forward(self, X):\n        \"\"\"\n        Args:\n            X: tensor of shape (n, d, d)\n        \"\"\"\n        X = self.flatten(X)\n        return self.fc(X)\n        # softmax is now computed in the loss function"]},{"cell_type":"markdown","metadata":{},"source":["Let us see the network in action. It inherits  a `__call__()` operator,\nwhich in turn calls the forward function (see below).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["model = LinearNetwork()\n\n# a random image\nX = torch.normal(0,1, size=(1, 28*28))\nprint(\"unnormalize probabilities :\\n\", model.forward(X))\n\nnp.testing.assert_almost_equal(\n    model.forward(X).detach().numpy(),\n    model(X).detach().numpy())"]},{"cell_type":"markdown","metadata":{},"source":["The parameters can be accessed using the `parameters()` method\nof the model object\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["for p in model.parameters():\n    print(p)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["for name, p in model.named_parameters():\n    print(name)\n    print(p)"]},{"cell_type":"markdown","metadata":{},"source":["This allows a standard way to optimize the parameters, for example:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# a batch of three random pictures\nloss_func = nn.CrossEntropyLoss()\nmodel = LinearNetwork(num_input=28*28, num_output=3)\n\nX = torch.normal(0,1, size=(3, 28*28))\ny = torch.tensor([1,0,2])\n\nloss = loss_func(model(X), y)\nloss.backward()\n\nlr = 1.0\n\nprint('before: fc.bias', model.get_parameter('fc.bias'))\nwith torch.no_grad():\n    for name, p in model.named_parameters():\n        p -= lr*p.grad # an inplace operation\n\nmodel.zero_grad()\nprint('after: fc.bias', model.get_parameter('fc.bias'))"]},{"cell_type":"markdown","metadata":{},"source":["The parameter update can be delegated to the `optim` package of PyTorch.\nAn example is shown here:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch.optim as optim\n\nX = torch.normal(0,1, size=(3, 4*4))\ny = torch.tensor([0, 1, 2])\n\nmodel = LinearNetwork(num_input=16, num_output=3)\n\nopt = optim.SGD(model.parameters(), lr=lr)\n\nfor _ in range(3):\n    loss = loss_func(model(X), y)\n    loss.backward()\n    opt.step()\n    model.zero_grad()\n\ny_hat = model(X)\n\nprint(\"Unnormalized probs \\n\", y_hat)\nprint()\nprint(\"Prediction\",y_hat.argmax(dim=1))"]},{"cell_type":"markdown","metadata":{},"source":["### Implement other metrics\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Often we are interested in the so called accuracy:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def accuracy(preds, truth):\n    return (truth == preds).float().mean()\n\nX = torch.normal(0,1, size=(3, 4*4))\ny = torch.tensor([0, 1, 2])\n\nmodel = LinearNetwork(num_input=16, num_output=3)\n\nopt = optim.SGD(model.parameters(), lr=lr)\n\nfor _ in range(2):\n    loss = loss_func(model(X), y)\n    loss.backward()\n    opt.step()\n    model.zero_grad()\n    with torch.no_grad():\n        acc = accuracy(model(X).argmax(dim=1), y )\n        print(f\"acc={acc}\")"]},{"cell_type":"markdown","metadata":{},"source":["Instead of our own implementation\nwe use the `torchmetrics` package (it provides some additional features, like aggregation over batches, see code below).\nInstall it with\n\n    pip install torchmetrics\n\nExample for global aggregation:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from torchmetrics import Accuracy\n\nmetric = Accuracy(task='multiclass', num_classes=4)\ny = torch.tensor([0, 1, 2])\np1 = torch.tensor([0, 1, 2])\np2 = torch.tensor([1, 0, 0])\n\nprint(\"batch #1\", metric(y, p1))\nprint(\"batch #2\", metric(y, p2))\nprint(\"global\", metric.compute())\nmetric.reset()"]},{"cell_type":"markdown","metadata":{},"source":["### Training Loop\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Lets train it with SGD but using the data loader. Also we split the training\ndata into a validation set and one real training data set.\nWe want to record the accuracy on the validation set and use Tensorboard for monitoring.\nHere is a helper function to set the path of Tensorboard.\nAdapt it to your own needs.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os.path\nimport datetime\nfrom pathlib import Path\n\ndef get_path(prefix='', name='', log_dir='tmp/tensorboard'):\n    if len(name)==0:\n        name = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    return os.path.join(Path.home(), log_dir, prefix, name)\n\nprint(get_path(prefix='advdm_mlp'))\nprint(get_path(prefix='advdm_mlp', name='exp1'))"]},{"cell_type":"markdown","metadata":{},"source":["OK, so let us start.\nThis could take a while, you can lower the number of epochs to speed it up.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n\nlr=0.05\nbatch_size = 256 #64\n\nn_epochs = 20\nN_train = 50000\nN_valid = 10000\n\n# speed up the data loader by prefetching\nfrom torch.utils.data import TensorDataset\n\nif not isinstance(fashion_train, TensorDataset):\n    fashion_train =  TensorDataset(fashion_train.data.float()/255.0, fashion_train.targets)\n    # this bypass the transform defined\n\ntrain_ds, valid_ds = torch.utils.data.random_split(fashion_train, [N_train, N_valid])\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n\nprint('Number of training data points', N_train)\nprint('Number of training batches', len(train_dl))\nprint('Number of validation data points', N_valid)\nprint('Number of validation batches', len(valid_dl))\n\nwriter = SummaryWriter(get_path('advdm_mlp'))\n#writer.add_text('Model', 'Linear model')\n\nmodel = LinearNetwork()\nopt = optim.SGD(model.parameters(), lr=lr)\n\nloss_func = nn.CrossEntropyLoss()\n\nstart = time.time()\nloss_train = 0\nloss_val = 0\nacc_val = 0\n\nmetric = Accuracy(task='multiclass', num_classes=10)\n\nfor epoch in range(n_epochs):\n    running_loss = 0.0\n    for X, y in train_dl:\n        model.zero_grad()\n        loss = loss_func(model(X), y)\n        loss.backward()\n        opt.step()\n        running_loss += loss.item()*X.shape[0]\n\n    loss_train = running_loss/N_train\n    writer.add_scalar(\"Loss/train\", loss_train, epoch)\n\n    with torch.no_grad():\n        loss_val=0\n        for X, y in valid_dl:\n            N = X.shape[0]\n            y_pred = model(X)\n            metric(y_pred, y)\n            loss_val += loss_func(model(X), y).item()*N\n\n    loss_val /= N_valid\n    acc_val =  metric.compute()\n    metric.reset()\n\n    writer.add_scalar(\"Loss/val\", loss_val, epoch)\n    writer.add_scalar(\"Accuracy/val\", acc_val, epoch)\n    writer.flush()\n\nwriter.add_hparams(\n    {'lr': lr, 'batch_size': batch_size, 'h_layers': 0},\n    {'hparam/loss_val': loss_val,\n     'hparam/loss_train': loss_train,\n     'hparam/acc_val': acc_val\n     })\n\nwriter.flush()\nwriter.close()\n\nprint(f'eplapsed time {time.time() - start:2.2f} sec')\nprint(f'loss train {loss_train:.2f}')\nprint(f'loss validation {loss_val:.2f}')"]},{"cell_type":"markdown","metadata":{},"source":["## Task: Build an MLP\n\n"]},{"cell_type":"markdown","metadata":{},"source":["So far we used a simple linear network for classification.\nYou should now add a hidden layer with 256 nodes and train and evaluate it.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["class MLP(nn.Module):\n    def __init__(self, num_input=28*28, num_hidden=256, num_output=10):\n        # TODO  impplement me\n        pass\n\n    def forward(self, X):\n        \"\"\"\n        Args:\n            X: tensor of shape (n, d)\n        \"\"\"\n       # TODO: Implement ,e\n       pass"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}