{"cells":[{"cell_type":"markdown","metadata":{},"source":["Networks with Multiple Layers\n","=============================\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Overview\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Here we will use  the **MNIST fashion** data set for classification, available here\n","[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)\n","or using `torchvision.datasets.FashionMNIST`.\n","\n","We will also monitor our model with the help of Tensorboard.\n","\n","We will start from a Softmax Regression which we construct from scratch.\n","This is them refactored to use the standard modules from `pytorch`, e.g.,\n","\n","-   `torch.nn.Module`, `torch.nn.Linear`, &#x2026;\n","-   `torch.optim` for optimization\n","-   `DataLoader`\n","\n","Finally you have to construct and train a MLP.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Imports\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["The *standard* imports:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","import torch\n","import torchvision"]},{"cell_type":"markdown","metadata":{},"source":["## Tensorboard\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["For the following, you have to install **Tensorboard**. First you need to install it with\n","\n","    pip install tensorboard\n","\n","You have to run it from the command line with\n","\n","    tensorboard --logdir <DIR>\n","\n","Later your models will write logging information to the directory `DIR`.\n","For now it is assumed that\n","\n","    DIR=$HOME/tmp/tensorboard\n","\n","Tensorboard can be accessed under [http://localhost:6006](http://localhost:6006).\n","\n","You will find more information here:\n","[https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### FashionMNIST\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Some info from [https://pytorch.org/docs/stable/torchvision/datasets.html](https://pytorch.org/docs/stable/torchvision/datasets.html)\n","> TORCHVISION.DATASETS\n","> All datasets are subclasses of `torch.utils.data.Dataset` i.e, they have `__getitem__` and `__len__` methods implemented.\n","> Hence, they can all be passed to a `torch.utils.data.DataLoader` which can load multiple samples\n","> parallelly using `torch.multiprocessing` workers.Here is the data set we want to use:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.datasets import FashionMNIST\n","# if you want, read the doc string:\n","# ?FashionMNIST"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision import transforms\n","transform = transforms.Compose([transforms.ToTensor()])\n","\n","fashion_train = FashionMNIST(\"~/no_backup/data_fashion/\",\n","               train=True, # the default\n","               download=True,\n","               transform=transform\n",")\n","\n","# this will will not be useed in to following\n","fashion_test = FashionMNIST(\"~/no_backup/data_fashion/\",\n","               train=False,\n","               download=True,\n","               transform=transform # converts to [0,1]\n",")\n","# if you like you can later evaluate your final model with it"]},{"cell_type":"markdown","metadata":{},"source":["Let us look at the data. Here are the classes:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fashion_train.classes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print (\"Type\", type(fashion_train.data))\n","print (\"dtype\", fashion_train.data.dtype)\n","print (\"Shape\", fashion_train.data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["So a big tensor, each picture is a 28x28 pixel picture.\n","But also note the `dtype` is a `torch.uint`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fashion_train.data.float().dtype"]},{"cell_type":"markdown","metadata":{},"source":["Lets us show some examples, together with their labels:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(3,3, figsize=(12,12))\n","axs = axs.flatten()\n","for i, ax in zip(range(len(axs)), axs):\n","    ax.imshow(fashion_train.data[i], cmap='gray')\n","    ax.set_title(fashion_train.classes[fashion_train.targets[i]])"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loaders\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["The data can be passed in a model with a `DataLoader`, like shown below.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","train_dl = DataLoader(fashion_train, batch_size=10, shuffle=True)\n","\n","print('Number of data points', len(fashion_train))\n","print ('Number of batches', len(train_dl))\n","\n","for X, y in train_dl:\n","    print (X.shape)\n","    print(y.shape)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["## Start from Softmax Regression\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Introduction\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["We will understand this as a classification problem with $28*28$ features and first apply Softmax regresssion.\n","Internally we need to  reshape the data. At the end we expect a $N\\times 784$ tensor,\n","where $N$ is the number of examples in the batch.\n","We see that `X` is of shape $N \\times d \\times d$, with $d=28$.\n","To flatten it (using the batch from the previous section) use for example\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X.flatten(start_dim=1).shape"]},{"cell_type":"markdown","metadata":{},"source":["We start from  our solution from last time:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def softmax_old(X):\n","    X_exp = torch.exp(X)\n","    partition = X_exp.sum(axis=1, keepdims=True)\n","    return X_exp / partition\n","\n","def cross_entropy_old(y_hat, y):\n","    return (-torch.log(y_hat[range(len(y_hat)), y])).mean()\n","\n","class SoftmaxNetwork_old:\n","\n","    def __init__(self, num_input, num_output, dtype=torch.float64):\n","        \"\"\"\n","        Args:\n","            num_input: dimension of input space\n","            num_output: number if output classes\n","        \"\"\"\n","        self.w = torch.randn((num_input,num_output),\n","                             dtype=dtype).requires_grad_(True)\n","        self.b = torch.randn(num_output, dtype=dtype).requires_grad_(True)\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Args:\n","            X: tensor of shape (n, d)\n","        \"\"\"\n","        y = (X @ self.w + self.b)\n","        return softmax_old(y)"]},{"cell_type":"markdown","metadata":{},"source":["### Refactor: Softmax and Cross-Entropy\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Basically an equivalent implementation of `Softmax` using PyTorch  functions is shown here:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import  torch.nn as nn\n","\n","softmax = nn.Softmax(dim=1)\n","\n","def test_softmax():\n","    X = torch.normal(0,1, size=(4,2))\n","    # would throw an exception if assertion is wrong\n","    np.testing.assert_almost_equal(\n","       softmax(X).numpy(), softmax_old(X) )\n","\n","test_softmax()"]},{"cell_type":"markdown","metadata":{},"source":["But due to numerical reasons it is recommended to use `LogSoftmax`:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["log_softmax = nn.LogSoftmax(dim=1)\n","\n","def test_softmax2():\n","    X = torch.normal(0,1, size=(4,2))\n","    # would throw an exception if assertion is wrong\n","    np.testing.assert_almost_equal(\n","       softmax(X).numpy(), np.exp(log_softmax(X)))\n","\n","test_softmax2()"]},{"cell_type":"markdown","metadata":{},"source":["Of course, now we have to change our loss function, which now works on the logarithm of the softmax:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test it\n","def test_cross_entropy():\n","    loss_func_old =cross_entropy_old\n","\n","    loss_func = nn.NLLLoss()\n","\n","    X = torch.normal(0,1, size=(4,2))\n","    y = torch.tensor([1,0,1,1])\n","\n","    np.testing.assert_almost_equal(\n","        loss_func(log_softmax(X), y).numpy(),\n","        loss_func_old(softmax_old(X), y).numpy()\n","    )\n","\n","\n","test_cross_entropy()"]},{"cell_type":"markdown","metadata":{},"source":["A second (equivalent method) is to use the `CrossEntropyLoss`:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_cross_entropy2():\n","    loss_func = nn.NLLLoss()\n","    loss_func_new = nn.CrossEntropyLoss()\n","\n","    # test it\n","    X = torch.normal(0,1, size=(4,2))\n","    y = torch.tensor([1,0,1,1])\n","\n","    # throws an exception if the assertion fails\n","    np.testing.assert_almost_equal(\n","        loss_func_new(X, y).numpy(),\n","        loss_func(log_softmax(X), y).numpy())\n","\n","test_cross_entropy2()"]},{"cell_type":"markdown","metadata":{},"source":["In this case the network has to return the result of the  output\n","layer without the softmax!\n","\n","Here is an example if a  network to be used with `NLLLoss`:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SoftmaxNetwork:\n","    def __init__(self):\n","        num_input=28*28\n","        num_output=10\n","        self.w = torch.randn((num_input,num_output),\n","                             dtype=torch.float32).requires_grad_(True)\n","        self.b = torch.randn(num_output, dtype=torch.float32).requires_grad_(True)\n","\n","    # New method: call operator\n","    def __call__(self, X):\n","        return self.forward(X)\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Args:\n","            X: tensor of shape (n, d, d)\n","        \"\"\"\n","        # flatten X!\n","        y = (X.flatten(start_dim=1) @ self.w + self.b)\n","        return log_softmax(y)"]},{"cell_type":"markdown","metadata":{},"source":["In the following section we will switch to the second possibility, therefore use the `CrossEntropyLoss`.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Refactor: Inherit from nn.Module\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Usually, PyTorch models inherit from `torch.nn.module`.\n","`torch.nn` contains many useful modules and layers.\n","For  example a simple **linear layer** (a **fully connected layer** where the activation\n","is the identity function), is provided by `nn.Linear`.\n","Also flattening can be done with `nn.Flatten`,\n","which can be viewed as a special layer.\n","\n","Using these new modules, we obtain the following class\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LinearNetwork(nn.Module):\n","    def __init__(self, num_input=28*28, num_output=10):\n","        # call constructor of super class\n","        super().__init__()\n","\n","        self.flatten = nn.Flatten()\n","        self.fc = nn.Linear(num_input, num_output)\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Args:\n","            X: tensor of shape (n, d, d)\n","        \"\"\"\n","        X = self.flatten(X)\n","        return self.fc(X)\n","        # softmax is now computed in the loss function"]},{"cell_type":"markdown","metadata":{},"source":["Let us see the network in action. It inherits  a `__call__()` operator,\n","which in turn calls the forward function (see below).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = LinearNetwork()\n","\n","# a random image\n","X = torch.normal(0,1, size=(1, 28*28))\n","print(\"unnormalize probabilities :\\n\", model.forward(X))\n","\n","np.testing.assert_almost_equal(\n","    model.forward(X).detach().numpy(),\n","    model(X).detach().numpy())"]},{"cell_type":"markdown","metadata":{},"source":["The parameters can be accessed using the `parameters()` method\n","of the model object\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for p in model.parameters():\n","    print(p)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for name, p in model.named_parameters():\n","    print(name)\n","    print(p)"]},{"cell_type":"markdown","metadata":{},"source":["This allows a standard way to optimize the parameters, for example:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# a batch of three random pictures\n","loss_func = nn.CrossEntropyLoss()\n","model = LinearNetwork(num_input=28*28, num_output=3)\n","\n","X = torch.normal(0,1, size=(3, 28*28))\n","y = torch.tensor([1,0,2])\n","\n","loss = loss_func(model(X), y)\n","loss.backward()\n","\n","lr = 1.0\n","\n","print('before: fc.bias', model.get_parameter('fc.bias'))\n","with torch.no_grad():\n","    for name, p in model.named_parameters():\n","        p -= lr*p.grad # an inplace operation\n","\n","model.zero_grad()\n","print('after: fc.bias', model.get_parameter('fc.bias'))"]},{"cell_type":"markdown","metadata":{},"source":["The parameter update can be delegated to the `optim` package of PyTorch.\n","An example is shown here:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.optim as optim\n","\n","X = torch.normal(0,1, size=(3, 4*4))\n","y = torch.tensor([0, 1, 2])\n","\n","model = LinearNetwork(num_input=16, num_output=3)\n","\n","opt = optim.SGD(model.parameters(), lr=lr) # stochastic gradient descent\n","\n","for _ in range(3):\n","    loss = loss_func(model(X), y)\n","    loss.backward()\n","    opt.step()\n","    model.zero_grad()\n","\n","y_hat = model(X)\n","\n","print(\"Unnormalized probs \\n\", y_hat)\n","print()\n","print(\"Prediction\",y_hat.argmax(dim=1))"]},{"cell_type":"markdown","metadata":{},"source":["### Implement other metrics\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Often we are interested in the so called accuracy:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def accuracy(preds, truth):\n","    return (truth == preds).float().mean()\n","\n","X = torch.normal(0,1, size=(3, 4*4))\n","y = torch.tensor([0, 1, 2])\n","\n","model = LinearNetwork(num_input=16, num_output=3)\n","\n","opt = optim.SGD(model.parameters(), lr=lr)\n","\n","for _ in range(2):\n","    loss = loss_func(model(X), y)\n","    loss.backward()\n","    opt.step()\n","    model.zero_grad()\n","    with torch.no_grad():\n","        acc = accuracy(model(X).argmax(dim=1), y )\n","        print(f\"acc={acc}\")"]},{"cell_type":"markdown","metadata":{},"source":["Instead of our own implementation\n","we use the `torchmetrics` package (it provides some additional features, like aggregation over batches, see code below).\n","Install it with\n","\n","    pip install torchmetrics\n","\n","Example for global aggregation:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchmetrics import Accuracy\n","\n","metric = Accuracy(task='multiclass', num_classes=4)\n","y = torch.tensor([0, 1, 2])\n","p1 = torch.tensor([0, 1, 2])\n","p2 = torch.tensor([1, 0, 0])\n","\n","print(\"batch #1\", metric(y, p1))\n","print(\"batch #2\", metric(y, p2))\n","print(\"global\", metric.compute())\n","metric.reset()"]},{"cell_type":"markdown","metadata":{},"source":["### Training Loop\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Lets train it with SGD but using the data loader. Also we split the training\n","data into a validation set and one real training data set.\n","We want to record the accuracy on the validation set and use Tensorboard for monitoring.\n","Here is a helper function to set the path of Tensorboard.\n","Adapt it to your own needs.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os.path\n","import datetime\n","from pathlib import Path\n","\n","def get_path(prefix='', name='', log_dir='log/tensorboard'):\n","    if len(name)==0:\n","        name = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","    return os.path.join(log_dir, prefix, name)\n","\n","print(get_path(prefix='advdm_mlp'))\n","print(get_path(prefix='advdm_mlp', name='exp1'))"]},{"cell_type":"markdown","metadata":{},"source":["OK, so let us start.\n","This could take a while, you can lower the number of epochs to speed it up.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","\n","lr=0.05\n","batch_size = 256 #64\n","\n","n_epochs = 20\n","N_train = 50000 # training set\n","N_valid = 10000 # validation set\n","\n","# speed up the data loader by prefetching\n","from torch.utils.data import TensorDataset\n","\n","if not isinstance(fashion_train, TensorDataset):\n","    fashion_train =  TensorDataset(fashion_train.data.float()/255.0, fashion_train.targets)\n","    # this bypass the transform defined\n","\n","train_ds, valid_ds = torch.utils.data.random_split(fashion_train, [N_train, N_valid])\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2)\n","\n","print('Number of training data points', N_train)\n","print('Number of training batches', len(train_dl))\n","print('Number of validation data points', N_valid)\n","print('Number of validation batches', len(valid_dl))\n","\n","writer = SummaryWriter(get_path('advdm_mlp'))\n","#writer.add_text('Model', 'Linear model')\n","\n","model = LinearNetwork()\n","opt = optim.SGD(model.parameters(), lr=lr)\n","\n","loss_func = nn.CrossEntropyLoss()\n","\n","start = time.time()\n","loss_train = 0\n","loss_val = 0\n","acc_val = 0\n","\n","metric = Accuracy(task='multiclass', num_classes=10)\n","\n","for epoch in range(n_epochs):\n","    running_loss = 0.0\n","    for X, y in train_dl:\n","        model.zero_grad()\n","        loss = loss_func(model(X), y)\n","        loss.backward()\n","        opt.step()\n","        running_loss += loss.item()*X.shape[0]\n","\n","    loss_train = running_loss/N_train\n","    writer.add_scalar(\"Loss/train\", loss_train, epoch)\n","\n","    with torch.no_grad():\n","        loss_val=0\n","        for X, y in valid_dl:\n","            N = X.shape[0]\n","            y_pred = model(X)\n","            metric(y_pred, y)\n","            loss_val += loss_func(model(X), y).item()*N\n","\n","    loss_val /= N_valid\n","    acc_val =  metric.compute()\n","    metric.reset()\n","\n","    writer.add_scalar(\"Loss/val\", loss_val, epoch)\n","    writer.add_scalar(\"Accuracy/val\", acc_val, epoch)\n","    writer.flush()\n","\n","writer.add_hparams(\n","    {'lr': lr, 'batch_size': batch_size, 'h_layers': 0},\n","    {'hparam/loss_val': loss_val,\n","     'hparam/loss_train': loss_train,\n","     'hparam/acc_val': acc_val\n","     })\n","\n","writer.flush()\n","writer.close()\n","\n","print(f'eplapsed time {time.time() - start:2.2f} sec')\n","print(f'loss train {loss_train:.2f}')\n","print(f'loss validation {loss_val:.2f}')"]},{"cell_type":"markdown","metadata":{},"source":["## Task: Build an MLP\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["So far we used a simple linear network for classification.\n","You should now add a hidden layer with 256 nodes and train and evaluate it.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, num_input=28*28, num_hidden=256, num_output=10):\n","        # TODO  impplement me\n","        pass\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Args:\n","            X: tensor of shape (n, d)\n","        \"\"\"\n","       # TODO: Implement ,e\n","       pass"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"org":null},"nbformat":4,"nbformat_minor":0}
