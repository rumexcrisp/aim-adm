{"cells":[{"cell_type":"markdown","metadata":{},"source":["Notebook Bag\n","============\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Preliminaries\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["This notebook requires the installation of the package `mimesis`.\n","Install it with `pip` or `conda`, for the latter run\n","`conda install -c conda-forge mimesis`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import platform\n","import os\n","\n","# Get the current operating system\n","os = platform.system()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Start the Dask Client\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Starting the Dask Client is optional. It will provide a dashboard which is useful to gain insight on the computation.\n","\n","The link to the dashboard will become visible when you create the client below. We recommend having it open on one side of your screen while using your notebook on the other side. This can take some effort to arrange your windows, but seeing them both at the same is very useful when learning.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from dask.distributed import Client, progress\n","\n","client = Client(n_workers=4, threads_per_worker=1)\n","client\n"]},{"cell_type":"markdown","metadata":{},"source":["Address of the scheduler\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["client.scheduler_info()\n"]},{"cell_type":"markdown","metadata":{},"source":["Notes:\n","\n","-   the connection string (ip:port) of the scheduler can be used to connect to an existing cluster\n","-   a cluster can be shutdown with `client.shutdown()`\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import dask\n","import json\n","import os\n","\n","os.makedirs(\"data/bag\", exist_ok=True)  # Create data/ directory\n","\n","b = dask.datasets.make_people(\n","    npartitions=10, records_per_partition=1000  # Make records of people,\n",")  # with default values\n","b.map(json.dumps).to_textfiles(\"data/bag/*.json\")  # Encode as JSON, write to disk\n"]},{"cell_type":"markdown","metadata":{},"source":["Take a quick look\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if os == 'Windows':\n","    !dir /b data\\bag\\*.json\n","elif os == 'Linux':\n","    !ls -lah data/bag/*.json"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["First look at the raw data:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if os == 'Windows':\n","    !Get-Content data\\bag\\0.json -TotalCount 2\n","elif os == 'Linux':\n","    !head -n 2 data/bag/0.json"]},{"cell_type":"markdown","metadata":{},"source":["Load it with dask:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import dask.bag as db\n","\n","b = db.read_text(\"data/bag/*.json\").map(json.loads)\n","b\n"]},{"cell_type":"markdown","metadata":{},"source":["How many entries are there?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["b.count().compute()\n"]},{"cell_type":"markdown","metadata":{},"source":["Take two elements (from the first partition)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["b.take(2)\n"]},{"cell_type":"markdown","metadata":{},"source":["Extract some information from each entry:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["b.map(lambda record: record[\"occupation\"]).take(2)\n"]},{"cell_type":"markdown","metadata":{},"source":["To get a list of all distinct occupations, use the function `distinct`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","b.map(lambda record: record['occupation']).distinct().compute()"]},{"cell_type":"markdown","metadata":{},"source":["What is the difference to this approach?\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","set(b.map(lambda record: record['occupation']).take(1000, npartitions=-1))"]},{"cell_type":"markdown","metadata":{},"source":["## Map, Filter, Aggregate\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["We can process this data by filtering out only certain records of interest, mapping functions over it to process our data, and aggregating those results to a total value.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["b.filter(lambda record: record[\"age\"] > 30).take(2)  # Select only people over 30\n"]},{"cell_type":"markdown","metadata":{},"source":["## Chain Computations\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["It is common to do many of these steps in one pipeline, only calling compute or take at the end.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result = (\n","    b.filter(lambda record: record[\"age\"] > 30)\n","    .map(lambda record: record[\"occupation\"])\n","    .frequencies(sort=True)\n","    .topk(15, key=1)\n",")\n","result\n"]},{"cell_type":"markdown","metadata":{},"source":["As with all lazy Dask collections, we need to call `compute` to actually evaluate our result. The `take` method used in earlier examples is also like `compute` and will also trigger computation.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["result.compute()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Convert to Dask DataFrames\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Dask Bags are good for reading in initial data, doing a bit of pre-processing, and then handing off to some other more efficient form like Dask Dataframes. Dask Dataframes use Pandas internally, and so can be much faster on numeric data and also have more complex algorithms.\n","\n","However, Dask Dataframes also expect data that is organized as flat columns. It does not support nested JSON data very well (Bag is better for this).\n","\n","Here we make a function to flatten down our nested data structure, map that across our records, and then convert that to a Dask Dataframe.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def flatten(record):\n","    return {\n","        \"age\": record[\"age\"],\n","        \"occupation\": record[\"occupation\"],\n","        \"telephone\": record[\"telephone\"],\n","        \"credit-card-number\": record[\"credit-card\"][\"number\"],\n","        \"credit-card-expiration\": record[\"credit-card\"][\"expiration-date\"],\n","        \"name\": \" \".join(record[\"name\"]),\n","        \"street-address\": record[\"address\"][\"address\"],\n","        \"city\": record[\"address\"][\"city\"],\n","    }\n","\n","\n","b.map(flatten).take(1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = b.map(flatten).to_dataframe()\n","df.head()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Task\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Count the number of people with expired credit cards.\n","Do not use the data frame form the previous task, instead use\n","the original bag `b` and apply a filter to it.\n","Finally provide a **pandas** DataFrame with columns\n","`name`, `street` and `city` that contains all these people\n","(Note: calling `compute` for a Dask DataFrame will return a Pandas DataFrame).\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"org":null},"nbformat":4,"nbformat_minor":0}
